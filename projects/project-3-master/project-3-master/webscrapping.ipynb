{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6c0b87-192f-46d8-86a2-d3dc40f73a69",
   "metadata": {},
   "source": [
    "# NLP modeling on baseball subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d87db-ef45-4a51-acd6-72f685759240",
   "metadata": {},
   "source": [
    "# Problem statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246228c-5a22-47de-9929-3bea23a6b3f0",
   "metadata": {},
   "source": [
    "Our client Major League Baseball (MLB) wants to improve their fan outreach. By getting fans excited about the game they hope to increase attendence and generate other types of revenue. They want to know if the same type of outreach will work for different types of fans. We will be comparing more analytically inclined fans to fans as a whole. We will create a model that tries to predict whether a post comes from the mlb subreddit or the sabermetrics subreddit. The mlb subreddit is for all MLB fans. The sabermetric subreddit is specifically for fans interested in a more analytical outlook. If the model can identify the differences that means that outreach should probably be tailored depending on which group it is for. And using the model we create MLB will be able to study the differences. For the model to be useful it will need to beat the baseline on new data. \n",
    "\n",
    "To create the model we will use ada boosting, random forests, logistic regression, and naive bayes. There are advantages to using these models. Logistic regression and naive bayes take relatively less computing power. It also tends not to overfit the data. Random Forests and adaboosting tend to perform well and correct overfitting issues in decision trees. The models will be evaluated using accuracy since one type of error is not any better or worse then the other in this case. We will also look at other metrics to get a full picture of how the model is doing.\n",
    "\n",
    "In addition for each model we will try the CountVectorizer and TfidfVectorizer to see which performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38578c6e-6287-4c96-84b3-7d600b3989ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b68ee4-4a35-439d-a0ee-54fc5615ebd6",
   "metadata": {},
   "source": [
    "Data will be scrapped from each subreddit. We are interested in what people are posting rather then the comments since those sometimes go off topic. Using the pushshift API library from pmaw allows us to read in the data faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273e819-30f6-4708-8a98-2f1630bb95d1",
   "metadata": {},
   "source": [
    "At first we were scrapping the same amount of data from both subreddits. However the mlb subreddit has more posts which are removed or deleted. In order to have more balanced classes we need to initially pull in more posts from the mlb subreddit. Once the data has been cleaned there will be a similar number of posts from both subreddits. There are arround 2250 posts in the sabermetric subreddit. By pulling 4500 posts from the mlb subreddit we achieve a good balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c1e5bfb4-17b9-4d7b-8efe-c125da11a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fa7f201b-56d7-4668-84ba-e1713bf308e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scrapper(subreddit): # recieved guidence on function from Tanveer Kahn and David Coons\n",
    "    api = PushshiftAPI() \n",
    "    posts = list(api.search_submissions(subreddit = subreddit, limit = 4500)) # using 4500 as the maximum led to fairly balanced data after preprocessing\n",
    "    df = pd.DataFrame(posts) \n",
    "    df = df[['selftext', 'subreddit']] #get the columns I am going to use in the models\n",
    "    df.to_csv(f'{subreddit}.csv',index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fba8ca28-47cf-4a88-8cd1-a8d2f548cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "data_scrapper('sabermetrics') # there are only about 2250 posts in this subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fbe925ea-7c63-4d18-a9bd-3ce8dcbeda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "data_scrapper('mlb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7286fde8-402c-48d7-a191-bb033a8ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('sabermetrics.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "db8807f0-c49f-4492-89f2-a117bd25e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('mlb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2c78296c-9830-4342-86ad-9d75ad97c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2]) # merge into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719120f7-9564-4c0f-9f35-4c5ce131b6e3",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd226f5-6312-4904-8be6-fa8de019b5b5",
   "metadata": {},
   "source": [
    "Now that the we have downloaded the data we have to clean it so it can be used in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8e714ea9-fc2c-4079-b086-efdf880b29cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6750 entries, 0 to 4499\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   selftext   3585 non-null   object\n",
      " 1   subreddit  6750 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 158.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #check the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf499d-5026-4002-8134-22bfcaf50670",
   "metadata": {},
   "source": [
    "There are over 6000 posts in the data. The subreddit category needs to be changed to a binary variable taking the value of 0 or 1 depending on which subreddit it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "8ae11ea5-dd59-426b-9d78-a1c8eb540a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map(lambda x: 0 if x == 'mlb' else 1) #from nlp lesson 2 code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82279daf-1423-422d-8acb-ca0d5f1ae927",
   "metadata": {},
   "source": [
    "Now the target variable is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5a95f43a-f6d7-47a0-baa5-3826188aafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['selftext'], inplace = True) # remove nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5451f155-1999-4b93-a8fb-0b4581646919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.selftext != '[deleted]'] #remove deleted posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a59e3cba-fee2-404d-adc1-8ef6494d4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.selftext != '[removed]'] # remove removed posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "14c0b7b5-2361-4088-a50c-9484cde6e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.selftext != ''] #remove blank posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c5b01fa0-13a6-4fc3-bba7-9dd9757e826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3091 entries, 1 to 4499\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   selftext   3091 non-null   object\n",
      " 1   subreddit  3091 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 72.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #cneck the data now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eef12-d028-45bd-ab12-3f3d1b940d2f",
   "metadata": {},
   "source": [
    "There are still over 3000 posts in our dataset. That should be enough to answer our question about whether we can predict which post a subreddit will belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5d13fd6e-3acc-403c-88a3-553d55100c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1568\n",
       "1    1523\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a984e-95d7-4939-9001-7d73b21981db",
   "metadata": {},
   "source": [
    "The value counts are very similar now. We will not have a serious issue with unbalanced classes. There are slightly more posts from the mlb subreddit so the baseline will be above 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8cd0acbe-5d27-4f5e-b74b-50c8cb333b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4ceff-bf3f-44b5-a900-1192c11a5822",
   "metadata": {},
   "source": [
    "Further analysis and modeling is in project 2 notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
